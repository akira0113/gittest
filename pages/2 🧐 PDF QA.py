import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Ollama & LanChainåŸºæœ¬
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

###### dotenv ã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„ ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################


def init_page():
    st.set_page_config(
        page_title="Ask My PDF(s)",
        page_icon="ğŸ§"
    )
    st.sidebar.title("Options")


def select_model(temperature=0):
    models = ("llama3.2", "gemmma2", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("Choose a model:", models)
    if model == "llama3.2":
        return ChatOllama(
        model="llama3.2",   #modelï¼ˆstrï¼‰: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã®åç§°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: "llama3.2" Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        temperature=temperature,      #temperatureï¼ˆfloatï¼‰: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å€¤ãŒä½ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šæ±ºå®šçš„ï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆï¼‰ã«ãªã‚Šã€é«˜ã„ã»ã©å‡ºåŠ›ã«å¤šæ§˜æ€§ãŒå¢—ã—ã¾ã™ã€‚
         # other params...
        )
    elif model == "gemma2":
        return ChatOllama(
        model="llama3.2",   #modelï¼ˆstrï¼‰: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã®åç§°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: "llama3.2" Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        temperature=temperature,      #temperatureï¼ˆfloatï¼‰: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å€¤ãŒä½ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šæ±ºå®šçš„ï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆï¼‰ã«ãªã‚Šã€é«˜ã„ã»ã©å‡ºåŠ›ã«å¤šæ§˜æ€§ãŒå¢—ã—ã¾ã™ã€‚
         # other params...
        )
    elif model == "Claude 3.5 Sonnet":
        return ChatOllama(
        model="llama3.2",   #modelï¼ˆstrï¼‰: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã®åç§°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: "llama3.2" Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        temperature=temperature,      #temperatureï¼ˆfloatï¼‰: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å€¤ãŒä½ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šæ±ºå®šçš„ï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆï¼‰ã«ãªã‚Šã€é«˜ã„ã»ã©å‡ºåŠ›ã«å¤šæ§˜æ€§ãŒå¢—ã—ã¾ã™ã€‚
         # other params...
        )
    elif model == "Gemini 1.5 Pro":
        return ChatOllama(
        model="llama3.2",   #modelï¼ˆstrï¼‰: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã®åç§°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: "llama3.2" Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        temperature=temperature,      #temperatureï¼ˆfloatï¼‰: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å€¤ãŒä½ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šæ±ºå®šçš„ï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆï¼‰ã«ãªã‚Šã€é«˜ã„ã»ã©å‡ºåŠ›ã«å¤šæ§˜æ€§ãŒå¢—ã—ã¾ã™ã€‚
         # other params...
        )


def init_qa_chain():
    llm = select_model()
    prompt = ChatPromptTemplate.from_template("""
    ä»¥ä¸‹ã®å‰æçŸ¥è­˜ã‚’ç”¨ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

    ===
    å‰æçŸ¥è­˜
    {context}

    ===
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
    {question}
    """)
    retriever = st.session_state.vectorstore.as_retriever(
        # "mmr",  "similarity_score_threshold" ãªã©ã‚‚ã‚ã‚‹
        search_type="similarity",
        # æ–‡æ›¸ã‚’ä½•å€‹å–å¾—ã™ã‚‹ã‹ (default: 4)
        search_kwargs={"k":10}
    )
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain


def page_ask_my_pdf():
    chain = init_qa_chain()

    if query := st.text_input("PDFã¸ã®è³ªå•ã‚’æ›¸ã„ã¦ã­: ", key="input"):
        st.markdown("## Answer")
        st.write_stream(chain.stream(query))


def main():
    init_page()
    st.title("PDF QA ğŸ§")
    if "vectorstore" not in st.session_state:
        st.warning("ã¾ãšã¯ ğŸ“„ Upload PDF(s) ã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã­")
    else:
        page_ask_my_pdf()


if __name__ == '__main__':
    main()
