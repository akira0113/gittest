import fitz  # PyMuPDF
import streamlit as st

# Ollama & LanChainåŸºæœ¬
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

###### dotenv ã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„ ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################


def init_page():
    st.set_page_config(
        page_title="Upload PDF(s)",
        page_icon="ğŸ“„"
    )
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("Clear DB", key="clear")
    if clear_button and "vectorstore" in st.session_state:
        del st.session_state.vectorstore


def get_pdf_text():
    # file_uploader ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    # (file_uploaderã®è©³ç´°ãªèª¬æ˜ã¯ç¬¬6ç« ã‚’ã”å‚ç…§ãã ã•ã„)
    pdf_file = st.file_uploader(
        label='Upload your PDF ğŸ˜‡',
        type='pdf'  # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯
    )
    if pdf_file:
        pdf_text = ""
        with st.spinner("Loading PDF ..."):
            # PyMuPDFã§PDFã‚’èª­ã¿å–ã‚‹
            # (è©³ç´°ãªèª¬æ˜ã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å…¬å¼ãƒšãƒ¼ã‚¸ãªã©ã‚’ã”å‚ç…§ãã ã•ã„)
            pdf_doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
            for page in pdf_doc:
                pdf_text += page.get_text()

        # RecursiveCharacterTextSplitter ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦åŠ¹ç‡çš„ã«åˆ†å‰²ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name="text-embedding-3-small",
            # é©åˆ‡ãª chunk size ã¯è³ªå•å¯¾è±¡ã®PDFã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ãŸã‚èª¿æ•´ãŒå¿…è¦
            # å¤§ããã—ã™ãã‚‹ã¨è³ªå•å›ç­”æ™‚ã«è‰²ã€…ãªç®‡æ‰€ã®æƒ…å ±ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ããªã„
            # é€†ã«å°ã•ã™ãã‚‹ã¨ä¸€ã¤ã®chunkã«ååˆ†ãªã‚µã‚¤ã‚ºã®æ–‡è„ˆãŒå…¥ã‚‰ãªã„
            # ãƒãƒ£ãƒ³ã‚¯ãŒå®Œå…¨ã«ç‹¬ç«‹ã—ãŸå½¢ã§åˆ†å‰²ã•ã‚Œã‚‹ã¨ã€åˆ†å‰²ç‚¹ä»˜è¿‘ã®æ–‡è„ˆï¼ˆå‰å¾Œé–¢ä¿‚ï¼‰ãŒå¤±ã‚ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã‚„ã‚·ã‚¹ãƒ†ãƒ ãŒé©åˆ‡ã«ç†è§£ã§ããªããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€overlapã‚’è¨­ã‘ã‚‹
            chunk_size=500,
            chunk_overlap=0,
        )
        return text_splitter.split_text(pdf_text)
    else:
        return None


def build_vector_store(pdf_text):
    with st.spinner("Saving to vector store ..."):
        if 'vectorstore' in st.session_state:
            st.session_state.vectorstore.add_texts(pdf_text)
        else:
            # PDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆ (pdf_text) ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€FAISSï¼ˆFacebook AI Similarity Searchï¼‰ã¨ã„ã†åŠ¹ç‡çš„ãªé¡ä¼¼æ¤œç´¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹
            # ãƒ™ã‚¯ãƒˆãƒ«DBã®åˆæœŸåŒ–ã¨æ–‡æ›¸ã®è¿½åŠ ã‚’åŒæ™‚ã«è¡Œã†
            # LangChain ã® Document Loader ã‚’åˆ©ç”¨ã—ãŸå ´åˆã¯ `from_documents` ã«ã™ã‚‹
            st.session_state.vectorstore = FAISS.from_texts(
                pdf_text,
                OpenAIEmbeddings(model="text-embedding-3-small")
            )

            # FAISSã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¯L2è·é›¢ã¨ãªã£ã¦ã„ã‚‹
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹
            # from langchain_community.vectorstores.utils import DistanceStrategy
            # st.session_state.vectorstore = FAISS.from_texts(
            #     pdf_text,
            #     OpenAIEmbeddings(model="text-embedding-3-small"),
            #     distance_strategy=DistanceStrategy.COSINE
            # )


def page_pdf_upload_and_build_vector_db():
    st.title("PDF Upload ğŸ“„")
    pdf_text = get_pdf_text()
    if pdf_text:
        build_vector_store(pdf_text)


def main():
    init_page()
    page_pdf_upload_and_build_vector_db()


if __name__ == '__main__':
    main()
