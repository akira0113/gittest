# ï¼’ç« ã‚¢ãƒ—ãƒª
import streamlit as st
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

#from langchain.prompts.chat import ChatPromptTemplate
from langchain.chains import LLMChain           #æœªä½¿ç”¨
from langchain_ollama.llms import OllamaLLM     #æœªä½¿ç”¨
from langchain_openai import ChatOpenAI         #æœªä½¿ç”¨

###### APIï¼¿KEYãªã©ã‚’ç’°å¢ƒå¤‰æ•°ã«å…¥ã‚Œã¦ã„ã‚‹å ´åˆã®èª­ã¿ã ã™å‡¦ç†ã€€dotenv ã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„ ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################


def main():
    st.set_page_config(
        page_title="ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ãƒãƒ£ãƒƒãƒˆ:llama3.2",
        page_icon="ğŸ¤—"
    )
    st.header("ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ãƒãƒ£ãƒƒãƒˆ:llama3.2")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–: message_history ãŒãªã‘ã‚Œã°ä½œæˆ
    if "message_history" not in st.session_state:
        st.session_state.message_history = [
            # System Prompt ã‚’è¨­å®š ('system' ã¯System Promptã‚’æ„å‘³ã™ã‚‹)
            ("system", "é–¢è¥¿å¼ã§ç­”ãˆã¦ãã ã•ã„ã€‚")
        ]

    # 1. LLMã«è³ªå•ã‚’ä¸ãˆã¦å›ç­”ã‚’å–ã‚Šå‡ºã™(ãƒ‘ãƒ¼ã‚¹ã™ã‚‹)å‡¦ç†ã‚’ä½œæˆ (1.-4.ã®å‡¦ç†)
    #llm = ChatOpenAI(temperature=0)
    #llm = OllamaLLM(model="llama3.2")
    llm = ChatOllama(
        model="llama3.2",   #modelï¼ˆstrï¼‰: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã®åç§°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ä¾‹: "llama3.2" Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
        temperature=0,      #temperatureï¼ˆfloatï¼‰: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã®æ¸©åº¦ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å€¤ãŒä½ã„ã»ã©å‡ºåŠ›ã¯ã‚ˆã‚Šæ±ºå®šçš„ï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ã‚’ç”Ÿæˆï¼‰ã«ãªã‚Šã€é«˜ã„ã»ã©å‡ºåŠ›ã«å¤šæ§˜æ€§ãŒå¢—ã—ã¾ã™ã€‚
    # other params...
    )

    # 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å—ã‘å–ã‚Šã€ChatGPTã«æ¸¡ã™ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    #    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã¯éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å«ã‚ã‚‹ã‚ˆã†ã«è¨­å®š
    prompt = ChatPromptTemplate.from_messages([
        *st.session_state.message_history,
        ("user", "{user_input}")  # ã“ã“ã«ã‚ã¨ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒå…¥ã‚‹
    ])

    # 3. ChatGPTã®è¿”ç­”ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ãŸã‚ã®å‡¦ç†ã‚’å‘¼ã³å‡ºã—
    output_parser = StrOutputParser()

    # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ChatGPTã«æ¸¡ã—ã€è¿”ç­”ã‚’å–ã‚Šå‡ºã™é€£ç¶šçš„ãªå‡¦ç†(chain)ã‚’ä½œæˆ
    #    å„è¦ç´ ã‚’ | (ãƒ‘ã‚¤ãƒ—) ã§ã¤ãªã’ã¦é€£ç¶šçš„ãªå‡¦ç†ã‚’ä½œæˆã™ã‚‹ã®ãŒLCELã®ç‰¹å¾´
    chain = prompt | llm | output_parser

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
        with st.spinner("LLM is typing ..."):
            response = chain.invoke({"user_input": user_input})

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ  ('user' ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’æ„å‘³ã™ã‚‹)
        st.session_state.message_history.append(("user", user_input))

        # ChatGPTã®å›ç­”ã‚’å±¥æ­´ã«è¿½åŠ  ('assistant' ã¯ChatGPTã®å›ç­”ã‚’æ„å‘³ã™ã‚‹)
        st.session_state.message_history.append(("ai", response))

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)


if __name__ == '__main__':
    main()
